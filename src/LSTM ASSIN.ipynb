{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM treinadas no conjunto ASSIN\n",
    "====\n",
    "\n",
    "Experimentos para treinar redes para a tarefa de RTE no conjunto ASSIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "embedding_file = '../data/embeddings.npy'\n",
    "vocabulary_file = '../data/vocabulary.txt'\n",
    "train_file = '../data/assin-ptbr-train.xml'\n",
    "dev_file = '../data/assin-ptbr-dev.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "====\n",
    "\n",
    "- Codificar de alguma forma quantidades. No modelo atual, embeddings mapeiam todos os dígitos para 9, de modo a se perder informações numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura dos dados e das embeddings\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import utils\n",
    "\n",
    "def load_dictionary(filename):\n",
    "    \"\"\"\n",
    "    Load a vocabulary file and create a dictionary.\n",
    "    \n",
    "    The rare or unknown symbol is taken to be the first in the list\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        text = unicode(f.read(), 'utf-8')\n",
    "    \n",
    "    words = text.splitlines()\n",
    "    mapping = zip(words[1:], range(1, len(words)))\n",
    "    \n",
    "    return defaultdict(int, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções auxiliares\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "max_sentence_len = 60\n",
    "\n",
    "def tokenize_sentence(text, change_quotes=True, change_digits=False):\n",
    "    '''\n",
    "    Tokenize the given sentence in Portuguese. The tokenization is done in conformity\n",
    "    with Universal Treebanks (at least it attempts so).\n",
    "    \n",
    "    :param change_quotes: if True, change different kinds of quotation marks to \"\n",
    "    :param change_digits: if True, replaces all digits with 9.\n",
    "    '''\n",
    "    if change_digits:\n",
    "        text = re.sub(r'\\d', '9', text)\n",
    "    \n",
    "    if change_quotes:\n",
    "        text = text.replace('“', '\"').replace('”', '\"')\n",
    "    \n",
    "    tokenizer_regexp = ur'''(?ux)\n",
    "    # the order of the patterns is important!!\n",
    "    (?:[^\\W\\d_]\\.)+|                  # one letter abbreviations, e.g. E.U.A.\n",
    "    \\d+(?:[.,]\\d+)*(?:[.,]\\d+)|       # numbers in format 999.999.999,99999\n",
    "    \\.{3,}|                           # ellipsis or sequences of dots\n",
    "    \\w+(?:\\.(?!\\.|$))?|               # words with numbers (including hours as 12h30), \n",
    "                                      # followed by a single dot but not at the end of sentence\n",
    "    \\d+:\\d+|                          # time and proportions\n",
    "    \\d+(?:[-\\\\/]\\d+)*|                # dates. 12/03/2012 12-03-2012\n",
    "    (?:[DSds][Rr][Aa]?)\\.|            # common abbreviations such as dr., sr., sra., dra.\n",
    "    \\$|                               # currency sign\n",
    "    (?:[\\#@]\\w+])|                    # Hashtags and twitter user names\n",
    "    -+|                               # any sequence of dashes\n",
    "    \\S                                # any non-space character\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(tokenizer_regexp)\n",
    "    \n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "def join_pairs(pairs, end_symbol):\n",
    "    \"\"\"\n",
    "    Join each pair of arrays in pairs in pairs and return a list of arrays.\n",
    "    \n",
    "    Each joined array has end_symbol between the contents of both original \n",
    "    arrays.\n",
    "    \"\"\"\n",
    "    new_pairs = []\n",
    "    for pair in pairs:\n",
    "        new_pair = np.concatenate([pair[0], [end_symbol], pair[1]])\n",
    "        new_pairs.append(new_pair)\n",
    "    \n",
    "    return new_pairs\n",
    "\n",
    "\n",
    "def create_batches(pairs, batch_size, padding_symbol, max_length=None):\n",
    "    \"\"\"\n",
    "    Create and return a list of tuples, each containing a batch and an array\n",
    "    with their actual sizes (without padding). Each batch is a numpy 2d \n",
    "    array in the format (max_length, batch_size)\n",
    "    \n",
    "    The pairs are first sorted by size, such as to minimize the amount of \n",
    "    padding needed.\n",
    "    \n",
    "    :param pairs: a list of arrays with token indices\n",
    "    :param max_length: if not None, batches are padded until they have this\n",
    "        length\n",
    "    \"\"\"\n",
    "    pairs.sort(key=lambda x: len(x))\n",
    "    num_batches = int(math.ceil(len(pairs) / batch_size))\n",
    "    batches = []\n",
    "    use_fixed_length = max_length is not None\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        ind_from = i * batch_size\n",
    "        ind_to = ind_from + batch_size\n",
    "        pairs_in_this_batch = pairs[ind_from:ind_to]\n",
    "        sequence_sizes = np.array([len(x) for x in pairs_in_this_batch])\n",
    "        \n",
    "        if not use_fixed_length:\n",
    "            # since the list is sorted by length, the last one is also the largest\n",
    "            max_length = len(pairs_in_this_batch[-1])\n",
    "        \n",
    "        # the first dimension might be smaller than batch_size for the last batch\n",
    "        shape = (len(pairs_in_this_batch), max_length)\n",
    "        batch = np.empty(shape, dtype=np.int32)\n",
    "        \n",
    "        for j in range(len(pairs_in_this_batch)):\n",
    "            # pad each array in the batch\n",
    "            # this for loop seems inneficient, but I couldn't come up\n",
    "            # with anything better\n",
    "            \n",
    "            pair = pairs_in_this_batch[j]\n",
    "            pad_shape = (0, max_length - len(pair))\n",
    "            \n",
    "            # the str call is because of a numpy bug\n",
    "            batch[j] = np.pad(pair, pad_shape, str('constant'), \n",
    "                              constant_values=padding_symbol)\n",
    "        \n",
    "        # append it as (max_length, batch_size)\n",
    "        batches.append((batch.T, sequence_sizes))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "def extract_raw_text(pairs, token_dict=None):\n",
    "    \"\"\"\n",
    "    Tokenize the T and H components of the given pairs, returning a list of tuples.\n",
    "    \n",
    "    If token_dict is not None, tokens are converted to their indices in the\n",
    "    embeddings space.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for pair in pairs:\n",
    "        tokens_t = tokenize_sentence(pair.t)\n",
    "        tokens_h = tokenize_sentence(pair.h)\n",
    "        \n",
    "        if token_dict is not None:\n",
    "            f = lambda tokens: np.array([token_dict[t.lower()] for t in tokens])\n",
    "            \n",
    "            indices_t = f(tokens_t)\n",
    "            indices_h = f(tokens_h)\n",
    "            item = (indices_t, indices_h)\n",
    "        else:\n",
    "            item = (tokens_t, tokens_h)\n",
    "        \n",
    "        result.append(item)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_dict = load_dictionary(vocabulary_file)\n",
    "\n",
    "train_pairs = utils.read_xml(train_file)\n",
    "dev_pairs = utils.read_xml(dev_file)\n",
    "\n",
    "train_pairs_text = extract_raw_text(train_pairs)\n",
    "dev_pairs_text = extract_raw_text(dev_pairs)\n",
    "\n",
    "train_pairs = extract_raw_text(train_pairs, token_dict)\n",
    "dev_pairs = extract_raw_text(dev_pairs, token_dict)\n",
    "\n",
    "embeddings = np.load(embedding_file)\n",
    "\n",
    "# the end symbol is the second to last one. It corresponds to the \"left padding\"\n",
    "# in the embeddings model\n",
    "end_symbol = max(token_dict.values()) - 1\n",
    "pad_symbol = max(token_dict.values())\n",
    "train_pairs = join_pairs(train_pairs, end_symbol)\n",
    "dev_pairs = join_pairs(dev_pairs, end_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = create_batches(train_pairs, 50, pad_symbol, max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "# lexsort picks the indices that sort the arrays following two or more criteria\n",
    "inds = np.lexsort((lengths_h, lengths_t))\n",
    "num_items = len(inds)\n",
    "\n",
    "plt.plot(np.arange(num_items), lengths_t[inds], 'r')\n",
    "plt.plot(np.arange(num_items), lengths_h[inds], 'b')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = lengths_t.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths_t[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo\n",
    "---\n",
    "\n",
    "A gloriosa LSTM! O primeiro modelo abaixo tenta simular uma versão mais simples da arquitetura usada por Rocktäschel et al. (2015), sem o modelo de atenção. É usada apenas uma LSTM. T e H são dadas como entrada sucessivamente, e um classificador sobre a saída a final da LSTM dá a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "embedding_size = 50\n",
    "num_lstm_units = 128\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # input placeholders\n",
    "    # ==================\n",
    "    # input_data has shape (sequence_length, batch_size) and each item is a token index\n",
    "    input_data = tf.placeholder(tf.int32, [max_sentence_len, None], name='token_indices')\n",
    "    input_labels = tf.placeholder(tf.int32, [None], name='labels')\n",
    "    \n",
    "    # the embeddings are a placeholder for pre-initialized values\n",
    "    tf_embeddings = tf.placeholder(tf.float32, [None, embedding_size])\n",
    "    \n",
    "    # we specify the length of each sentence in the batch\n",
    "    sentence_lengths = tf.placeholder(tf.int32, [None], name='sentence_lengths')\n",
    "    \n",
    "    # model parameters\n",
    "    # ================\n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(num_lstm_units, embedding_size)\n",
    "    \n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([num_lstm_units, num_classes], 0.0, 0.1))\n",
    "    softmax_bias = tf.Variable(tf.zeros([num_classes]))\n",
    "    \n",
    "    # model computations\n",
    "    # ==================\n",
    "    input_embeddings = tf.nn.embedding_lookup(tf_embeddings, input_data)\n",
    "    input_list = [tf.squeeze(time_step) \n",
    "                  for time_step in tf.split(0, max_sentence_len, input_embeddings)]\n",
    "    \n",
    "    lstm_outputs, state = tf.nn.rnn(lstm_cell, input_list, dtype=tf.float32, sequence_length=sentence_lengths)\n",
    "    \n",
    "    # lstm_outputs is (max_seq_len, batch, lstm_size)\n",
    "    # we only need the last output of each batch item\n",
    "    lstm_outputs = tf.pack(lstm_outputs, 'lstm_output')\n",
    "    \n",
    "    inds = tf.concat(1, [tf.expand_dims(sentence_lengths, 1), tf.expand_dims(tf.range(50), 1)])\n",
    "    print(inds.get_shape())\n",
    "    \n",
    "    last_outputs = tf.slice(lstm_outputs, )\n",
    "    \n",
    "    print(lstm_outputs.get_shape())\n",
    "    print(last_outputs.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução\n",
    "---\n",
    "\n",
    "Instanciação e execução do grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    batches = create_batches(train_pairs, 50, pad_symbol, max_sentence_len)\n",
    "\n",
    "    # test with 1 batch\n",
    "    batch, sequence_lengths_in_batch = batches[0]\n",
    "    \n",
    "    feeds = {input_data: batch, sentence_lengths: sequence_lengths_in_batch,\n",
    "             input_labels: np.arange(10), tf_embeddings: embeddings}\n",
    "    np_outputs, np_state = sess.run([lstm_outputs, state], feed_dict=feeds)\n",
    "    np_inds = sess.run([inds], feed_dict=feeds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_outputs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}