{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric LSTM\n",
    "=====\n",
    "\n",
    "Essa é a implementação de uma LSTM para ser treinada com operações aritméticas (a princípio, soma e subtração). Números são lidos um dígito de cada vez, de modo que a LSTM aprenda uma representação interna para quantidades. Espera-se que o aprendizado sobre como representar quantidades seja alcançado através do treinamento para operações aritméticas.\n",
    "\n",
    "Funcionamento geral\n",
    "----\n",
    "\n",
    "1. LSTM encoder codifica dois números\n",
    "\n",
    "2. Camada linear (ou relu) aplicada sobre a concatenação dos dois números para fazer a soma sobre a representação codificada. Deveria haver um conjunto de pesos para cada operação suportada\n",
    "\n",
    "3. LSTM decoder gera a saída, um algarismo de cada vez\n",
    "\n",
    "Obs:\n",
    "\n",
    "- A mesma LSTM é usada para codificar os dois números de entrada, sendo resetada após o primeiro\n",
    "\n",
    "- Embeddings são compartilhadas, mas o encoder e decoder têm seus próprios parâmetros\n",
    "\n",
    "Ideias relevantes para avaliação\n",
    "----\n",
    "\n",
    "O objetivo aqui é a geração de embeddings para números arbitrários. É desejável ter uma avaliação que mostre se essas embeddings realmente são úteis quando combinadas com um modelo de espaço vetorial para um vocabulário normal. Algumas ideias:\n",
    "\n",
    "- Gerar artificialmente várias sentenças com contruções do tipo \"há 20 pessoas\", \"há mais de 10 pessoas\" e tentar fazer algum tipo de classificação, como RTE\n",
    "\n",
    "- Avaliar essas embeddings em algum corpus de RTE onde haja muitos números, e contrastar com outras variantes que não tenham as embeddings numéricas. Um problema seria com o que comparar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "class Symbol(object):\n",
    "    \"\"\"\n",
    "    Placeholder class for values used in the RNNs.\n",
    "    \"\"\"\n",
    "    END = 10\n",
    "    GO = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geração de batches\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_array(x):\n",
    "    \"\"\"\n",
    "    This is slower than array to int\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return np.array([0])\n",
    "    \n",
    "    digits = []\n",
    "    while x > 0:\n",
    "        digits.insert(0, x % 10)\n",
    "        x /= 10\n",
    "    \n",
    "    return np.array(digits)\n",
    "\n",
    "\n",
    "def batch_array_to_ints(x):\n",
    "    \"\"\"\n",
    "    Return an array containing the integers represented by each\n",
    "    row of the input array.\n",
    "    \"\"\"\n",
    "    total = np.zeros_like(x[0])\n",
    "    for row in x:\n",
    "        total = 10 * total + row\n",
    "    return total\n",
    "\n",
    "\n",
    "def array_to_int(x):\n",
    "    total = 0\n",
    "    for i in x:\n",
    "        total = 10 * total + i\n",
    "    return total\n",
    "\n",
    "\n",
    "def ints_to_array_batch(x, max_time_steps):\n",
    "    \"\"\"\n",
    "    Takes an integer 1-D array and returns a 2-D array with their digits.\n",
    "    \n",
    "    The most significant figures are at the index 0\n",
    "    \n",
    "    :param x: 1-D integer array\n",
    "    :param max_time_steps: the maximum possible number of time steps\n",
    "    :return: 2-D array with shape (max_time_steps, len(x)) and one digit per cell.\n",
    "    \"\"\"\n",
    "    x = np.copy(x)\n",
    "    max_power_10 = max_time_steps - 2\n",
    "    \n",
    "    # since the last row of the resulting array must be the END symbol,\n",
    "    # this is the maximum number allowed\n",
    "    max_possible_number = 10 ** (max_time_steps - 1) - 1\n",
    "    assert not any(x > max_possible_number), 'Cannot represent a number in the array'\n",
    "    \n",
    "    new = np.full((max_time_steps, len(x)), Symbol.END, np.int32)\n",
    "    # just to make sure in case of the results is 0\n",
    "    new[0] = 0\n",
    "    \n",
    "    # start from 2 so that the last row always has END\n",
    "    for i in range(2, max_time_steps + 1):\n",
    "        big_ones = x >= (10 ** max_power_10)\n",
    "        new[-i][big_ones] = x[big_ones] % 10\n",
    "        x[big_ones] /= 10\n",
    "        \n",
    "        # on a related note: raising a variable to a power is faster than \n",
    "        # dividing it by 10\n",
    "        max_power_10 -= 1\n",
    "    \n",
    "    return new\n",
    "\n",
    "\n",
    "def ints_to_array_batch_reversed(x):\n",
    "    x = np.copy(x)\n",
    "    new = np.full((8, len(x)), 10, np.int32)\n",
    "    # just to make sure in case the result is 0\n",
    "    new[0] = 0\n",
    "\n",
    "    for i in range(8):\n",
    "        new[i][x > 0] = x[x > 0] % 10\n",
    "        x /= 10\n",
    "    \n",
    "    return new\n",
    "\n",
    "\n",
    "def generate_numbers(num_time_steps, num_padding, batch_size):\n",
    "    \"\"\"\n",
    "    Generate a 2-D array containing digits, and a corresponding\n",
    "    array with their concatenation.\n",
    "    \n",
    "    Each number will have the determined number of valid time steps,\n",
    "    plus the given number of padding (as the END symbol)\n",
    "    \"\"\"\n",
    "    shape = (num_time_steps, batch_size)\n",
    "    digits = np.random.random_integers(0, 9, shape)\n",
    "    numbers = batch_array_to_ints(digits)\n",
    "    padding = np.full((num_padding, batch_size), \n",
    "                      Symbol.END, np.int32)\n",
    "    digits = np.concatenate([digits, padding])\n",
    "    \n",
    "    return (digits, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    digits_pool = []\n",
    "    number_pool = []\n",
    "    \n",
    "    for i in range(1, input_sequence_size):\n",
    "        # we generate 10 1-digit numbers, 20 2-digit, 30 3-digit etc\n",
    "        num_padding = input_sequence_size - i\n",
    "        digits, numbers = generate_numbers(i, num_padding, i * 10)\n",
    "        digits_pool.append(digits)\n",
    "        number_pool.append(numbers)\n",
    "    \n",
    "    all_digits = np.concatenate(digits_pool, 1)\n",
    "    all_numbers = np.concatenate(number_pool)\n",
    "    \n",
    "    # shuffle both lists with the same permutation\n",
    "    rng_state = np.random.get_state()\n",
    "    # shuffle works row-wise, but we want to shuffle columns\n",
    "    np.random.shuffle(all_digits.T)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(all_numbers)\n",
    "    \n",
    "    # now divide it in the middle to get the two terms\n",
    "    middle = all_digits.shape[1] / 2\n",
    "    first_terms = all_digits[:, :middle]\n",
    "    first_terms_values = all_numbers[:middle]\n",
    "    second_terms = all_digits[:, middle:]\n",
    "    second_terms_values = all_numbers[middle:]\n",
    "    \n",
    "    results_integer = first_terms_values + second_terms_values\n",
    "    results = ints_to_array_batch(results_integer, output_sequence_size)\n",
    "    first_sizes = np.sum(first_terms != 10, 0)\n",
    "    second_sizes = np.sum(second_terms != 10, 0)\n",
    "    \n",
    "    return (first_terms, second_terms, \n",
    "            first_sizes, second_sizes,\n",
    "            results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do grafo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código abaixo reseta o grafo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "num_lstm_units = 100\n",
    "\n",
    "# a number with 8 digits at most\n",
    "input_sequence_size = 8\n",
    "\n",
    "# a number with 9 digits at most plus at least one END symbol\n",
    "output_sequence_size = 10\n",
    "\n",
    "# digits 0-9, GO and END symbols\n",
    "vocab_size = 12\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "# both terms are inputs to the encoder\n",
    "first_term = tf.placeholder(tf.int32, [input_sequence_size, None], 'first_term')\n",
    "second_term = tf.placeholder(tf.int32, [input_sequence_size, None], 'second_term')\n",
    "result_data = tf.placeholder(tf.int32, [output_sequence_size, None], 'result')\n",
    "\n",
    "first_term_size = tf.placeholder(tf.int32, [None], 'first_term_size')\n",
    "second_term_size = tf.placeholder(tf.int32, [None], 'second_term_size')\n",
    "\n",
    "l2_constant = tf.placeholder(tf.float32, 1, name='l2_constant')\n",
    "\n",
    "# we want to share the embeddings between encoder and decoder, but not all parameters\n",
    "shape = [vocab_size, embedding_size]\n",
    "embeddings = tf.Variable(tf.random_uniform(shape, -1.0, 1.0), name='embeddings')\n",
    "\n",
    "lstm_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(num_lstm_units, embedding_size, \n",
    "                                    initializer=lstm_initializer)\n",
    "\n",
    "\n",
    "def generate_rnn_input(sequence_indices, num_time_steps):\n",
    "    \"\"\"\n",
    "    Generate the input to the RNN from a tensor of shape [sequence_size, batch_size]\n",
    "    Return a list of tensors of shape [batch_size, embedding_size]\n",
    "    \"\"\"\n",
    "    embedded_sequence =  tf.nn.embedding_lookup(embeddings, sequence_indices)\n",
    "    return [tf.squeeze(time_step, [0]) \n",
    "            for time_step in tf.split(0, num_time_steps, embedded_sequence)]\n",
    "    \n",
    "input_1st_term = generate_rnn_input(first_term, input_sequence_size)\n",
    "input_2nd_term = generate_rnn_input(second_term, input_sequence_size)\n",
    "\n",
    "# create a tensor with the GO embedding with batch size\n",
    "embedded_go = tf.nn.embedding_lookup(embeddings, Symbol.GO)\n",
    "batch_go = tf.ones_like(input_1st_term[0]) * embedded_go\n",
    "\n",
    "result_labels = [tf.squeeze(time_step, [0]) \n",
    "                 for time_step in tf.split(0, output_sequence_size, result_data)]\n",
    "embedded_results = generate_rnn_input(result_data, output_sequence_size)\n",
    "decoder_input = [batch_go] + embedded_results[:-1]\n",
    "\n",
    "with tf.variable_scope('encoder') as encoder_scope:\n",
    "    _, state_1st_term = tf.nn.rnn(lstm_cell, input_1st_term, \n",
    "                                  sequence_length=first_term_size, dtype=tf.float32)\n",
    "    encoder_scope.reuse_variables()\n",
    "    _, state_2nd_term = tf.nn.rnn(lstm_cell, input_2nd_term, \n",
    "                                  sequence_length=second_term_size, dtype=tf.float32)\n",
    "\n",
    "two_terms = tf.concat(1, [state_1st_term, state_2nd_term])\n",
    "    \n",
    "with tf.variable_scope('addition_layer') as addition_scope:\n",
    "    # the addition layer has as input a concatenation of two encoded arrays\n",
    "    # its output is the input for the decoder LSTM\n",
    "    shape = [2 * lstm_cell.state_size, lstm_cell.state_size]\n",
    "    addition_weights = tf.Variable(tf.truncated_normal(shape, 0.0, 0.1))\n",
    "    addition_bias = tf.Variable(tf.zeros([lstm_cell.state_size]))\n",
    "    \n",
    "addition_output = tf.nn.xw_plus_b(two_terms, addition_weights, addition_bias)\n",
    "\n",
    "with tf.variable_scope('decoder') as decoder_scope:\n",
    "    decoder_outputs, _ = tf.nn.seq2seq.rnn_decoder(decoder_input, addition_output, lstm_cell)\n",
    "    \n",
    "with tf.variable_scope('output_softmax') as softmax_scope:\n",
    "    # softmax to map decoder raw output to digits\n",
    "    shape = [num_lstm_units, vocab_size]\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal(shape, 0.0, 0.1))\n",
    "    softmax_bias = tf.Variable(tf.zeros([vocab_size]))    \n",
    "\n",
    "def project_output(raw_outputs, return_softmax=False):\n",
    "    \"\"\"\n",
    "    Multiply the raw_outputs by a weight matrix, add a bias and return the\n",
    "    softmax distribution or the logits.\n",
    "    \n",
    "    :param return_softmax: if True, return the softmaxes. If False, return\n",
    "        the logits\n",
    "    \"\"\"\n",
    "    output_logits = [tf.nn.xw_plus_b(time_step, softmax_weights, softmax_bias)\n",
    "                     for time_step in raw_outputs]\n",
    "    if not return_softmax:\n",
    "        return output_logits\n",
    "    \n",
    "    output_softmax = [tf.nn.softmax(time_step) for time_step in output_logits]\n",
    "    return output_softmax\n",
    "\n",
    "# label_weights is just used to weight the importance of each class\n",
    "label_weights = [tf.ones_like(result_labels[0], dtype=tf.float32)\n",
    "                 for _ in result_labels]\n",
    "\n",
    "output_logits = project_output(decoder_outputs, False)\n",
    "l2_loss = l2_constant * (tf.nn.l2_loss(softmax_weights) + tf.nn.l2_loss(addition_weights))\n",
    "loss = tf.nn.seq2seq.sequence_loss(output_logits, result_labels, label_weights) + l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starting_learning_rate = 0.2\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#learning_rate = tf.train.exponential_decay(starting_learning_rate, global_step, 10000, 0.98)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(starting_learning_rate, epsilon=0.1)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\n",
    "train_op = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                     global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução de novos inputs\n",
    "---\n",
    "\n",
    "Roda o encoder, a soma e o decoder por um passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use the same intermediate results of the training part until the addition\n",
    "# the difference is that the decoder must use its own output\n",
    "\n",
    "digit_step = tf.placeholder(tf.int32, [None], 'digit_step')\n",
    "decoder_step_state = tf.placeholder(tf.float32, [None, lstm_cell.state_size], \n",
    "                                    'decoder_step_state')\n",
    "\n",
    "# embed the input digit\n",
    "decoder_step_input = [tf.nn.embedding_lookup(embeddings, digit_step)]\n",
    "\n",
    "\n",
    "with tf.variable_scope(decoder_scope) as exec_time_decoder:\n",
    "    exec_time_decoder.reuse_variables()\n",
    "    decoder_step_output, decoder_new_state = tf.nn.seq2seq.rnn_decoder(decoder_step_input, \n",
    "                                                                       decoder_step_state, lstm_cell)\n",
    "    \n",
    "    step_logits = tf.nn.xw_plus_b(decoder_step_output[0], softmax_weights, softmax_bias)\n",
    "    next_symbol = tf.argmax(step_logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "sample_first_term = batch[0][:, n].reshape((-1, 1))\n",
    "sample_second_term = batch[1][:, n].reshape((-1, 1))\n",
    "sample_first_size =  batch[2][n].reshape((1))\n",
    "sample_second_size =  batch[3][n].reshape((1))\n",
    "sample_result = batch[4][:, n].reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feeds = {first_term: sample_first_term,\n",
    "         second_term: sample_second_term,\n",
    "         first_term_size: sample_first_size,\n",
    "         second_term_size: sample_second_size}\n",
    "sample_decoder_state = sess.run(addition_output, feed_dict=feeds)\n",
    "\n",
    "answer = []\n",
    "current_symbol = [Symbol.GO]\n",
    "while True:\n",
    "    decoder_feeds = {decoder_step_state: sample_decoder_state,\n",
    "                     digit_step: current_symbol}\n",
    "    \n",
    "    fetches = sess.run([next_symbol, decoder_new_state], \n",
    "                       feed_dict=decoder_feeds)\n",
    "    current_symbol, sample_decoder_state = fetches\n",
    "    \n",
    "    if current_symbol == Symbol.END:\n",
    "        break\n",
    "    \n",
    "    answer.append(current_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45057\n",
      "449571010101010\n"
     ]
    }
   ],
   "source": [
    "print(''.join(str(x[0]) for x in answer))\n",
    "print(''.join(str(x) for x in sample_result.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Loss: 1.50703\n",
      "Epoch 200\n",
      "Loss: 1.41706\n",
      "Epoch 300\n",
      "Loss: 1.36825\n",
      "Epoch 400\n",
      "Loss: 1.35879\n",
      "Epoch 500\n",
      "Loss: 1.31487\n",
      "Epoch 600\n",
      "Loss: 1.27562\n",
      "Epoch 700\n",
      "Loss: 1.24197\n",
      "Epoch 800\n",
      "Loss: 1.19104\n",
      "Epoch 900\n",
      "Loss: 1.16172\n",
      "Epoch 1000\n",
      "Loss: 1.12861\n",
      "Epoch 1100\n",
      "Loss: 1.11705\n",
      "Epoch 1200\n",
      "Loss: 1.09908\n",
      "Epoch 1300\n",
      "Loss: 1.08800\n",
      "Epoch 1400\n",
      "Loss: 1.04927\n",
      "Epoch 1500\n",
      "Loss: 1.02802\n",
      "Epoch 1600\n",
      "Loss: 1.03931\n",
      "Epoch 1700\n",
      "Loss: 1.03907\n",
      "Epoch 1800\n",
      "Loss: 1.03325\n",
      "Epoch 1900\n",
      "Loss: 0.99064\n",
      "Epoch 2000\n",
      "Loss: 0.98467\n",
      "Epoch 2100\n",
      "Loss: 0.97119\n",
      "Epoch 2200\n",
      "Loss: 0.97819\n",
      "Epoch 2300\n",
      "Loss: 0.97482\n",
      "Epoch 2400\n",
      "Loss: 0.96442\n",
      "Epoch 2500\n",
      "Loss: 0.94674\n",
      "Epoch 2600\n",
      "Loss: 0.96394\n",
      "Epoch 2700\n",
      "Loss: 0.95011\n",
      "Epoch 2800\n",
      "Loss: 0.92620\n",
      "Epoch 2900\n",
      "Loss: 0.94196\n",
      "Epoch 3000\n",
      "Loss: 0.95102\n",
      "Epoch 3100\n",
      "Loss: 0.93077\n",
      "Epoch 3200\n",
      "Loss: 0.91106\n",
      "Epoch 3300\n",
      "Loss: 0.90492\n",
      "Epoch 3400\n",
      "Loss: 0.92678\n",
      "Epoch 3500\n",
      "Loss: 0.91659\n",
      "Epoch 3600\n",
      "Loss: 0.90096\n",
      "Epoch 3700\n",
      "Loss: 0.88646\n",
      "Epoch 3800\n",
      "Loss: 0.90194\n",
      "Epoch 3900\n",
      "Loss: 0.90068\n",
      "Epoch 4000\n",
      "Loss: 0.87564\n",
      "Epoch 4100\n",
      "Loss: 0.87883\n",
      "Epoch 4200\n",
      "Loss: 0.87894\n",
      "Epoch 4300\n",
      "Loss: 0.87726\n",
      "Epoch 4400\n",
      "Loss: 0.85927\n",
      "Epoch 4500\n",
      "Loss: 0.87638\n",
      "Epoch 4600\n",
      "Loss: 0.86552\n",
      "Epoch 4700\n",
      "Loss: 0.83991\n",
      "Epoch 4800\n",
      "Loss: 0.83010\n",
      "Epoch 4900\n",
      "Loss: 0.81493\n",
      "Epoch 5000\n",
      "Loss: 0.77091\n",
      "Epoch 5100\n",
      "Loss: 0.76205\n",
      "Epoch 5200\n",
      "Loss: 0.74009\n",
      "Epoch 5300\n",
      "Loss: 0.71391\n",
      "Epoch 5400\n",
      "Loss: 0.72724\n",
      "Epoch 5500\n",
      "Loss: 0.70377\n",
      "Epoch 5600\n",
      "Loss: 0.70490\n",
      "Epoch 5700\n",
      "Loss: 0.69681\n",
      "Epoch 5800\n",
      "Loss: 0.69489\n",
      "Epoch 5900\n",
      "Loss: 0.68760\n",
      "Epoch 6000\n",
      "Loss: 0.66681\n",
      "Epoch 6100\n",
      "Loss: 0.65217\n",
      "Epoch 6200\n",
      "Loss: 0.64140\n",
      "Epoch 6300\n",
      "Loss: 0.68683\n",
      "Epoch 6400\n",
      "Loss: 0.64683\n",
      "Epoch 6500\n",
      "Loss: 0.64648\n",
      "Epoch 6600\n",
      "Loss: 0.65094\n",
      "Epoch 6700\n",
      "Loss: 0.61344\n",
      "Epoch 6800\n",
      "Loss: 0.62544\n",
      "Epoch 6900\n",
      "Loss: 0.60640\n",
      "Epoch 7000\n",
      "Loss: 0.61129\n",
      "Epoch 7100\n",
      "Loss: 0.60666\n",
      "Epoch 7200\n",
      "Loss: 0.58372\n",
      "Epoch 7300\n",
      "Loss: 0.58381\n",
      "Epoch 7400\n",
      "Loss: 0.56613\n",
      "Epoch 7500\n",
      "Loss: 0.54732\n",
      "Epoch 7600\n",
      "Loss: 0.54840\n",
      "Epoch 7700\n",
      "Loss: 0.52621\n",
      "Epoch 7800\n",
      "Loss: 0.53929\n",
      "Epoch 7900\n",
      "Loss: 0.52466\n",
      "Epoch 8000\n",
      "Loss: 0.50258\n",
      "Epoch 8100\n",
      "Loss: 0.53962\n",
      "Epoch 8200\n",
      "Loss: 0.47026\n",
      "Epoch 8300\n",
      "Loss: 0.46965\n",
      "Epoch 8400\n",
      "Loss: 0.47469\n",
      "Epoch 8500\n",
      "Loss: 0.50217\n",
      "Epoch 8600\n",
      "Loss: 0.50921\n",
      "Epoch 8700\n",
      "Loss: 0.44366\n",
      "Epoch 8800\n",
      "Loss: 0.45933\n",
      "Epoch 8900\n",
      "Loss: 0.43837\n",
      "Epoch 9000\n",
      "Loss: 0.46676\n",
      "Epoch 9100\n",
      "Loss: 0.44239\n",
      "Epoch 9200\n",
      "Loss: 0.45453\n",
      "Epoch 9300\n",
      "Loss: 0.44488\n",
      "Epoch 9400\n",
      "Loss: 0.43034\n",
      "Epoch 9500\n",
      "Loss: 0.45146\n",
      "Epoch 9600\n",
      "Loss: 0.43825\n",
      "Epoch 9700\n",
      "Loss: 0.45719\n",
      "Epoch 9800\n",
      "Loss: 0.41237\n",
      "Epoch 9900\n",
      "Loss: 0.42861\n",
      "Epoch 10000\n",
      "Loss: 0.42207\n",
      "Epoch 10100\n",
      "Loss: 0.42590\n",
      "Epoch 10200\n",
      "Loss: 0.40887\n",
      "Epoch 10300\n",
      "Loss: 0.42139\n",
      "Epoch 10400\n",
      "Loss: 0.40322\n",
      "Epoch 10500\n",
      "Loss: 0.39951\n",
      "Epoch 10600\n",
      "Loss: 0.39318\n",
      "Epoch 10700\n",
      "Loss: 0.39175\n",
      "Epoch 10800\n",
      "Loss: 0.38202\n",
      "Epoch 10900\n",
      "Loss: 0.38605\n",
      "Epoch 11000\n",
      "Loss: 0.40758\n",
      "Epoch 11100\n",
      "Loss: 0.38448\n",
      "Epoch 11200\n",
      "Loss: 0.39453\n",
      "Epoch 11300\n",
      "Loss: 0.35989\n",
      "Epoch 11400\n",
      "Loss: 0.37685\n",
      "Epoch 11500\n",
      "Loss: 0.35407\n",
      "Epoch 11600\n",
      "Loss: 0.38035\n",
      "Epoch 11700\n",
      "Loss: 0.38768\n",
      "Epoch 11800\n",
      "Loss: 0.33904\n",
      "Epoch 11900\n",
      "Loss: 0.33271\n",
      "Epoch 12000\n",
      "Loss: 0.33636\n",
      "Epoch 12100\n",
      "Loss: 0.35015\n",
      "Epoch 12200\n",
      "Loss: 0.31922\n",
      "Epoch 12300\n",
      "Loss: 0.32298\n",
      "Epoch 12400\n",
      "Loss: 0.32651\n",
      "Epoch 12500\n",
      "Loss: 0.34670\n",
      "Epoch 12600\n",
      "Loss: 0.31761\n",
      "Epoch 12700\n",
      "Loss: 0.30745\n",
      "Epoch 12800\n",
      "Loss: 0.31125\n",
      "Epoch 12900\n",
      "Loss: 0.32220\n",
      "Epoch 13000\n",
      "Loss: 0.30438\n",
      "Epoch 13100\n",
      "Loss: 0.30909\n",
      "Epoch 13200\n",
      "Loss: 0.31181\n",
      "Epoch 13300\n",
      "Loss: 0.30198\n",
      "Epoch 13400\n",
      "Loss: 0.31092\n",
      "Epoch 13500\n",
      "Loss: 0.31032\n",
      "Epoch 13600\n",
      "Loss: 0.31800\n",
      "Epoch 13700\n",
      "Loss: 0.29693\n",
      "Epoch 13800\n",
      "Loss: 0.26858\n",
      "Epoch 13900\n",
      "Loss: 0.28494\n",
      "Epoch 14000\n",
      "Loss: 0.25992\n",
      "Epoch 14100\n",
      "Loss: 0.29879\n",
      "Epoch 14200\n",
      "Loss: 0.27498\n",
      "Epoch 14300\n",
      "Loss: 0.29253\n",
      "Epoch 14400\n",
      "Loss: 0.29035\n",
      "Epoch 14500\n",
      "Loss: 0.25845\n",
      "Epoch 14600\n",
      "Loss: 0.28597\n",
      "Epoch 14700\n",
      "Loss: 0.26668\n",
      "Epoch 14800\n",
      "Loss: 0.27232\n",
      "Epoch 14900\n",
      "Loss: 0.26188\n",
      "Epoch 15000\n",
      "Loss: 0.25638\n",
      "Epoch 15100\n",
      "Loss: 0.25293\n",
      "Epoch 15200\n",
      "Loss: 0.27172\n",
      "Epoch 15300\n",
      "Loss: 0.26294\n",
      "Epoch 15400\n",
      "Loss: 0.23266\n",
      "Epoch 15500\n",
      "Loss: 0.23675\n",
      "Epoch 15600\n",
      "Loss: 0.25654\n",
      "Epoch 15700\n",
      "Loss: 0.23056\n",
      "Epoch 15800\n",
      "Loss: 0.24963\n",
      "Epoch 15900\n",
      "Loss: 0.21414\n",
      "Epoch 16000\n",
      "Loss: 0.25041\n",
      "Epoch 16100\n",
      "Loss: 0.23282\n",
      "Epoch 16200\n",
      "Loss: 0.24625\n",
      "Epoch 16300\n",
      "Loss: 0.24519\n",
      "Epoch 16400\n",
      "Loss: 0.23084\n",
      "Epoch 16500\n",
      "Loss: 0.23791\n",
      "Epoch 16600\n",
      "Loss: 0.23768\n",
      "Epoch 16700\n",
      "Loss: 0.23676\n",
      "Epoch 16800\n",
      "Loss: 0.22738\n",
      "Epoch 16900\n",
      "Loss: 0.23183\n",
      "Epoch 17000\n",
      "Loss: 0.22072\n",
      "Epoch 17100\n",
      "Loss: 0.23253\n",
      "Epoch 17200\n",
      "Loss: 0.20784\n",
      "Epoch 17300\n",
      "Loss: 0.22287\n",
      "Epoch 17400\n",
      "Loss: 0.22236\n",
      "Epoch 17500\n",
      "Loss: 0.21508\n",
      "Epoch 17600\n",
      "Loss: 0.20092\n",
      "Epoch 17700\n",
      "Loss: 0.21912\n",
      "Epoch 17800\n",
      "Loss: 0.20710\n",
      "Epoch 17900\n",
      "Loss: 0.21973\n",
      "Epoch 18000\n",
      "Loss: 0.19910\n",
      "Epoch 18100\n",
      "Loss: 0.19915\n",
      "Epoch 18200\n",
      "Loss: 0.20502\n",
      "Epoch 18300\n",
      "Loss: 0.20806\n",
      "Epoch 18400\n",
      "Loss: 0.20807\n",
      "Epoch 18500\n",
      "Loss: 0.20868\n",
      "Epoch 18600\n",
      "Loss: 0.19042\n",
      "Epoch 18700\n",
      "Loss: 0.20298\n",
      "Epoch 18800\n",
      "Loss: 0.20875\n",
      "Epoch 18900\n",
      "Loss: 0.18895\n",
      "Epoch 19000\n",
      "Loss: 0.20514\n",
      "Epoch 19100\n",
      "Loss: 0.19574\n",
      "Epoch 19200\n",
      "Loss: 0.21198\n",
      "Epoch 19300\n",
      "Loss: 0.20470\n",
      "Epoch 19400\n",
      "Loss: 0.19159\n",
      "Epoch 19500\n",
      "Loss: 0.19139\n",
      "Epoch 19600\n",
      "Loss: 0.20414\n",
      "Epoch 19700\n",
      "Loss: 0.20295\n",
      "Epoch 19800\n",
      "Loss: 0.19122\n",
      "Epoch 19900\n",
      "Loss: 0.19123\n",
      "Epoch 20000\n",
      "Loss: 0.21107\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(0, 20000):\n",
    "    batch = generate_data()\n",
    "    feeds = {first_term: batch[0], \n",
    "             second_term: batch[1],\n",
    "             first_term_size: batch[2],\n",
    "             second_term_size: batch[3],\n",
    "             result_data: batch[4],\n",
    "             l2_constant: [1e-5]}\n",
    "    \n",
    "    try:\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feeds)\n",
    "    except:\n",
    "        add = addition_output.eval(feed_dict=feeds, session=sess)\n",
    "        print(add)\n",
    "        print(add.shape)\n",
    "        raise\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('Epoch %d' % (i + 1))\n",
    "        print('Loss: %.5f' % loss_value)\n",
    "\n",
    "# sample_1st_term = np.random.random_integers(0, 9, (20, 1))\n",
    "# test_feeds = {sample_input: sample_sequence, sample_size: [6]}\n",
    "# np_encoded_state = sess.run([sample_encoder_state], feed_dict=test_feeds)[0]\n",
    "\n",
    "# chosen_digit = [Symbol.GO]\n",
    "# answer = []\n",
    "\n",
    "# while len(answer) < max_sample_size:\n",
    "#     decoder_feeds = {sample_decoder_input: [chosen_digit],\n",
    "#                      sample_decoder_state: np_encoded_state}\n",
    "#     np_softmax, np_encoded_state = sess.run([sample_softmax[0], new_decoder_state],\n",
    "#                                              feed_dict=decoder_feeds)\n",
    "#     chosen_digit = np_softmax.argmax(1)\n",
    "    \n",
    "#     if chosen_digit == Symbol.END:\n",
    "#         break\n",
    "#     else:\n",
    "#         answer.append(chosen_digit)\n",
    "    \n",
    "\n",
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}