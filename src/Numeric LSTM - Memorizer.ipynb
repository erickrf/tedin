{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric LSTM\n",
    "=====\n",
    "\n",
    "Essa é a implementação de uma LSTM memorizar uma representação de números de vários algarismos. Números são lidos um dígito de cada vez, de modo que a LSTM aprenda uma representação interna para quantidades. Em seguida, uma outra LSTM decodifica a célula de memória, gerando os números originais.\n",
    "\n",
    "Funcionamento geral\n",
    "----\n",
    "\n",
    "1. LSTM encoder codifica a sequência de números.\n",
    "\n",
    "2. LSTM decoder gera a saída, um algarismo de cada vez (é aplicado um softmax sobre a saída da rede)\n",
    "\n",
    "Obs:\n",
    "\n",
    "- Embeddings são compartilhadas, mas o encoder e decoder têm seus próprios parâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a number with 8 digits most and END\n",
    "output_sequence_size = 9\n",
    "\n",
    "# digits 0-9\n",
    "encoder_vocab = 10\n",
    "\n",
    "# digits 0-9 and END symbol\n",
    "decoder_vocab = 11\n",
    "\n",
    "# digits 0-9, GO and END symbols\n",
    "vocab_size = 12\n",
    "\n",
    "class Symbol(object):\n",
    "    \"\"\"\n",
    "    Placeholder class for values used in the RNNs.\n",
    "    \"\"\"\n",
    "    END = 10\n",
    "    GO = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do grafo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código abaixo reseta o grafo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "num_lstm_units = embedding_size\n",
    "\n",
    "# a number with up to 9 digits\n",
    "input_sequence_size = 9\n",
    "\n",
    "max_sample_size = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "first_term = tf.placeholder(tf.int32, [input_sequence_size, None], 'first_term')\n",
    "first_term_size = tf.placeholder(tf.int32, [None], 'first_term_size')\n",
    "\n",
    "# we want to share the embeddings between encoder and decoder, but not all parameters\n",
    "shape = [vocab_size, embedding_size]\n",
    "embeddings = tf.Variable(tf.random_uniform(shape, -1.0, 1.0), name='embeddings')\n",
    "\n",
    "lstm_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(num_lstm_units, embedding_size, \n",
    "                                    initializer=lstm_initializer)\n",
    "\n",
    "with tf.variable_scope('output_softmax') as softmax_scope:\n",
    "    # softmax to map decoder raw output to digits\n",
    "    shape = [num_lstm_units, decoder_vocab]\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal(shape, 0.0, 0.1))\n",
    "    softmax_bias = tf.Variable(tf.zeros([vocab_size]))    \n",
    "\n",
    "def generate_rnn_input(sequence_indices, num_time_steps):\n",
    "    \"\"\"\n",
    "    Generate the embedding input to the RNN from a tensor\n",
    "    of shape [sequence_size, batch_size]\n",
    "    \n",
    "    Return a list of tensors of shape [batch_size, embedding_size]\n",
    "    \"\"\"\n",
    "    embedded_sequence =  tf.nn.embedding_lookup(embeddings, sequence_indices)\n",
    "    return [tf.squeeze(time_step, [0]) \n",
    "            for time_step in tf.split(0, num_time_steps, embedded_sequence)]\n",
    "    \n",
    "\n",
    "input_1st_term = generate_rnn_input(first_term, input_sequence_size)\n",
    "\n",
    "with tf.variable_scope('encoder') as encoder_scope:\n",
    "    _, state_1st_term = tf.nn.rnn(lstm_cell, input_1st_term, \n",
    "                                  sequence_length=first_term_size, dtype=tf.float32)\n",
    "\n",
    "# create a tensor of 1's with the appropriate size and then multiply it by GO embeddings\n",
    "ones = tf.ones_like(input_1st_term[0])\n",
    "embedded_go = tf.nn.embedding_lookup(embeddings, Symbol.GO)\n",
    "batch_embedded_go = ones * embedded_go\n",
    "\n",
    "input_as_list = [tf.squeeze(time_step)\n",
    "                 for time_step in tf.split(0, input_sequence_size, first_term)]\n",
    "\n",
    "decoder_inputs = [batch_embedded_go] + input_1st_term\n",
    "\n",
    "# the END symbol is just a label; doesn't need embeddings\n",
    "ones = tf.ones_like(input_as_list[0])\n",
    "batch_end = ones * Symbol.END\n",
    "decoder_labels = input_as_list + [batch_end]\n",
    "\n",
    "# label_weights is just used to weight the importance of each class\n",
    "label_weights = [tf.ones_like(decoder_labels[0], dtype=tf.float32)\n",
    "                 for _ in decoder_labels]\n",
    "\n",
    "with tf.variable_scope('decoder') as decoder_scope:\n",
    "    raw_outputs, _ = tf.nn.seq2seq.rnn_decoder(decoder_inputs, state_1st_term, \n",
    "                                               lstm_cell)\n",
    "\n",
    "def project_output(raw_outputs, return_softmax=False):\n",
    "    \"\"\"\n",
    "    Multiply the raw_outputs by a weight matrix, add a bias and return the\n",
    "    softmax distribution or the logits.\n",
    "    \n",
    "    :param return_softmax: if True, return the softmaxes. If False, return\n",
    "        the logits\n",
    "    \"\"\"\n",
    "    output_logits = [tf.nn.xw_plus_b(time_step, softmax_weights, softmax_bias)\n",
    "                     for time_step in raw_outputs]\n",
    "    \n",
    "    if not return_softmax:\n",
    "        return output_logits\n",
    "    \n",
    "    output_softmax = [tf.nn.softmax(time_step) for time_step in output_logits]\n",
    "    return output_softmax\n",
    "\n",
    "output_logits = project_output(raw_outputs, False)\n",
    "loss = tf.nn.seq2seq.sequence_loss(output_logits, decoder_labels, label_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=0.1)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\n",
    "train_op = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                     global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of new inputs \n",
    "---\n",
    "\n",
    "Run the encoder for one single step.\n",
    "\n",
    "`next_symbol` and `decoder_new_state` should be called for each step inside a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use the same intermediate results of the training part until the \n",
    "# encoder hidden state\n",
    "digit_step = tf.placeholder(tf.int32, [None], 'digit_step')\n",
    "decoder_step_state = tf.placeholder(tf.float32, [None, lstm_cell.state_size], \n",
    "                                    'decoder_step_state')\n",
    "\n",
    "# embed the input digits\n",
    "decoder_step_input = [tf.nn.embedding_lookup(embeddings, digit_step)]\n",
    "\n",
    "with tf.variable_scope(decoder_scope) as exec_time_decoder:\n",
    "    exec_time_decoder.reuse_variables()\n",
    "    decoder_step_output, decoder_new_state = tf.nn.seq2seq.rnn_decoder(decoder_step_input, \n",
    "                                                                       decoder_step_state, lstm_cell)\n",
    "    \n",
    "    step_logits = tf.nn.xw_plus_b(decoder_step_output[0], softmax_weights, softmax_bias)\n",
    "    next_symbol = tf.argmax(step_logits, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sequences(sequence_size, batch_size):\n",
    "    \"\"\"\n",
    "    Generate a sequence of numbers as an array.\n",
    "    \n",
    "    All sequences must have the same size.\n",
    "    \n",
    "    :param sequence_size: the number of items (digits) actually\n",
    "        contained in the sequences\n",
    "    :param batch_size: number of sequences\n",
    "    \"\"\"\n",
    "    dims = (sequence_size, batch_size)\n",
    "    sequences = np.random.random_integers(0, 9, dims)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def extract_answer(input_sequence, sequence_size):\n",
    "    \"\"\"\n",
    "    Create a numpy array with the decoder outputs\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    batch_size = input_sequence.shape[1]\n",
    "    current_symbol = [Symbol.GO] * batch_size\n",
    "    \n",
    "    encoder_feeds = {first_term: input_sequence, \n",
    "                     first_term_size: sequence_size}\n",
    "    hidden_state = sess.run([state_1st_term], feed_dict=encoder_feeds)[0]\n",
    "    \n",
    "    # this array control which sequences have already been finished by the\n",
    "    # decoder, i.e., for which ones it already produced the END symbol\n",
    "    sequences_done = np.zeros(batch_size, dtype=np.bool)\n",
    "    while True:\n",
    "        decoder_feeds = {decoder_step_state: hidden_state,\n",
    "                         digit_step: current_symbol}\n",
    "\n",
    "        fetches = sess.run([next_symbol, decoder_new_state], \n",
    "                           feed_dict=decoder_feeds)\n",
    "        current_symbol, hidden_state = fetches\n",
    "        \n",
    "        sequences_done |= (current_symbol == Symbol.END)\n",
    "                \n",
    "        if sequences_done.all():\n",
    "            break\n",
    "\n",
    "        answer.append(current_symbol)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_sizes = np.array([9] * batch_size)\n",
    "answer = np.array(extract_answer(random_valid, sequence_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80381944444444442"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(answer == random_valid) / answer.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Loss: 2.23064\n",
      "Epoch 100\n",
      "Loss: 1.90105\n",
      "Epoch 150\n",
      "Loss: 1.71675\n",
      "Epoch 200\n",
      "Loss: 1.59983\n",
      "Epoch 250\n",
      "Loss: 1.44190\n",
      "Epoch 300"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "batch_size = 64\n",
    "\n",
    "accumulated_loss = 0\n",
    "report_interval = 50\n",
    "\n",
    "for i in range(5000):\n",
    "    \n",
    "    # a single size for all items in the input\n",
    "    sequences = generate_sequences(input_sequence_size, batch_size)\n",
    "    sequence_size = np.random.random_integers(1, input_sequence_size)\n",
    "    sequence_size_array = np.array([sequence_size] * batch_size)\n",
    "    \n",
    "    feeds = {first_term: sequences, first_term_size: sequence_size_array}\n",
    "\n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict=feeds)\n",
    "    accumulated_loss += loss_value\n",
    "    \n",
    "    if (i + 1) % report_interval == 0:\n",
    "        print('Epoch %d' % (i + 1))\n",
    "        avg_loss = accumulated_loss / report_interval\n",
    "        print('Loss: %.5f' % avg_loss)\n",
    "        accumulated_loss = 0\n",
    "\n",
    "#     random_valid = generate_sequences(input_sequence_size, batch_size)\n",
    "#     valid_feeds = {first_term: random_valid, first_term_size: input_sequence_size}\n",
    "#     np_encoded_state = sess.run([sample_encoder_state], feed_dict=test_feeds)[0]\n",
    "\n",
    "\n",
    "\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_valid = generate_sequences(input_sequence_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
