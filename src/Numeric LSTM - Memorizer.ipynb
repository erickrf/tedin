{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric LSTM\n",
    "=====\n",
    "\n",
    "Essa é a implementação de uma LSTM memorizar uma representação de números de vários algarismos. Números são lidos um dígito de cada vez, de modo que a LSTM aprenda uma representação interna para quantidades. Em seguida, uma outra LSTM decodifica a célula de memória, gerando os números originais.\n",
    "\n",
    "Funcionamento geral\n",
    "----\n",
    "\n",
    "1. LSTM encoder codifica a sequência de números.\n",
    "\n",
    "2. LSTM decoder gera a saída, um algarismo de cada vez (é aplicado um softmax sobre a saída da rede)\n",
    "\n",
    "Obs:\n",
    "\n",
    "- Embeddings são compartilhadas, mas o encoder e decoder têm seus próprios parâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a number with 8 digits most and END\n",
    "output_sequence_size = 9\n",
    "\n",
    "# digits 0-9\n",
    "encoder_vocab = 10\n",
    "\n",
    "# digits 0-9 and END symbol\n",
    "decoder_vocab = 11\n",
    "\n",
    "# digits 0-9, GO and END symbols\n",
    "vocab_size = 12\n",
    "\n",
    "class Symbol(object):\n",
    "    \"\"\"\n",
    "    Placeholder class for values used in the RNNs.\n",
    "    \"\"\"\n",
    "    END = 10\n",
    "    GO = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do grafo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código abaixo reseta o grafo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "num_lstm_units = embedding_size\n",
    "\n",
    "# a number with up to 9 digits\n",
    "input_sequence_size = 9\n",
    "\n",
    "max_sample_size = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "first_term = tf.placeholder(tf.int32, [input_sequence_size, None], 'first_term')\n",
    "first_term_size = tf.placeholder(tf.int32, [None], 'first_term_size')\n",
    "\n",
    "# we want to share the embeddings between encoder and decoder, but not all parameters\n",
    "shape = [vocab_size, embedding_size]\n",
    "embeddings = tf.Variable(tf.random_uniform(shape, -1.0, 1.0), name='embeddings')\n",
    "\n",
    "lstm_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(num_lstm_units, embedding_size, \n",
    "                                    initializer=lstm_initializer)\n",
    "\n",
    "with tf.variable_scope('output_softmax') as softmax_scope:\n",
    "    # softmax to map decoder raw output to digits\n",
    "    shape = [num_lstm_units, decoder_vocab]\n",
    "    initializer = tf.truncated_normal_initializer(0.0, 0.1)\n",
    "    softmax_weights = tf.get_variable('weights', shape, tf.float32, initializer)\n",
    "    \n",
    "    initializer = tf.zeros_initializer([decoder_vocab])\n",
    "    softmax_bias = tf.get_variable('bias', initializer=initializer)\n",
    "\n",
    "def generate_rnn_input(sequence_indices, num_time_steps):\n",
    "    \"\"\"\n",
    "    Generate the embedding input to the RNN from a tensor\n",
    "    of shape [sequence_size, batch_size].\n",
    "    \n",
    "    If trim is not None, reduce the first dimension of the sequences\n",
    "    to the given value.\n",
    "    \n",
    "    Return a list of tensors of shape [batch_size, embedding_size]\n",
    "    \"\"\"\n",
    "    embedded_sequence =  tf.nn.embedding_lookup(embeddings, sequence_indices)\n",
    "    return [tf.squeeze(time_step, [0]) \n",
    "            for time_step in tf.split(0, num_time_steps, embedded_sequence)]\n",
    "    \n",
    "\n",
    "input_1st_term = generate_rnn_input(first_term, input_sequence_size)\n",
    "\n",
    "with tf.variable_scope('encoder') as encoder_scope:\n",
    "    _, state_1st_term = tf.nn.rnn(lstm_cell, input_1st_term, \n",
    "                                  sequence_length=first_term_size, dtype=tf.float32)\n",
    "\n",
    "# create a tensor of 1's with the appropriate size and then multiply it by GO embeddings\n",
    "ones = tf.ones_like(input_1st_term[0])\n",
    "embedded_go = tf.nn.embedding_lookup(embeddings, Symbol.GO)\n",
    "batch_embedded_go = ones * embedded_go\n",
    "\n",
    "input_as_list = [tf.squeeze(time_step)\n",
    "                 for time_step in tf.split(0, input_sequence_size, first_term)]\n",
    "decoder_inputs = [batch_embedded_go] + input_1st_term\n",
    "\n",
    "# the END symbol is just a label; doesn't need embeddings\n",
    "ones = tf.ones_like(input_as_list[0])\n",
    "batch_end = ones * Symbol.END\n",
    "decoder_labels = input_as_list + [batch_end]\n",
    "\n",
    "# label_weights is just used to weight the importance of each class\n",
    "label_weights = [tf.ones_like(decoder_labels[0], dtype=tf.float32)\n",
    "                 for _ in decoder_labels]\n",
    "\n",
    "with tf.variable_scope('decoder') as decoder_scope:\n",
    "    raw_outputs, _ = tf.nn.seq2seq.rnn_decoder(decoder_inputs, state_1st_term, \n",
    "                                               lstm_cell)\n",
    "\n",
    "def project_output(raw_outputs, return_softmax=False):\n",
    "    \"\"\"\n",
    "    Multiply the raw_outputs by a weight matrix, add a bias and return the\n",
    "    softmax distribution or the logits.\n",
    "    \n",
    "    :param return_softmax: if True, return the softmaxes. If False, return\n",
    "        the logits\n",
    "    \"\"\"\n",
    "    output_logits = [tf.nn.xw_plus_b(time_step, softmax_weights, softmax_bias)\n",
    "                     for time_step in raw_outputs]\n",
    "    \n",
    "    if not return_softmax:\n",
    "        return output_logits\n",
    "    \n",
    "    output_softmax = [tf.nn.softmax(time_step) for time_step in output_logits]\n",
    "    return output_softmax\n",
    "\n",
    "output_logits = project_output(raw_outputs, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "l2_constant = tf.placeholder(tf.float32, name='l2_constant')\n",
    "\n",
    "labeled_loss = tf.nn.seq2seq.sequence_loss(output_logits, decoder_labels, label_weights)\n",
    "l2_loss = l2_constant * tf.nn.l2_loss(softmax_weights)\n",
    "loss = labeled_loss + l2_loss\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=0.1)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\n",
    "train_op = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                     global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of new inputs \n",
    "---\n",
    "\n",
    "Run the encoder for one single step.\n",
    "\n",
    "`next_symbol` and `decoder_new_state` should be called for each step inside a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use the same intermediate results of the training part until the \n",
    "# encoder hidden state\n",
    "digit_step = tf.placeholder(tf.int32, [None], 'digit_step')\n",
    "decoder_step_state = tf.placeholder(tf.float32, [None, lstm_cell.state_size], \n",
    "                                    'decoder_step_state')\n",
    "\n",
    "# embed the input digits\n",
    "decoder_step_input = [tf.nn.embedding_lookup(embeddings, digit_step)]\n",
    "\n",
    "with tf.variable_scope(decoder_scope) as exec_time_decoder:\n",
    "    exec_time_decoder.reuse_variables()\n",
    "    decoder_step_output, decoder_new_state = tf.nn.seq2seq.rnn_decoder(decoder_step_input, \n",
    "                                                                       decoder_step_state, lstm_cell)\n",
    "    \n",
    "    step_logits = tf.nn.xw_plus_b(decoder_step_output[0], softmax_weights, softmax_bias)\n",
    "    next_symbol = tf.argmax(step_logits, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sequences(array_size, sequence_size, batch_size):\n",
    "    \"\"\"\n",
    "    Generate a sequence of numbers as an array.\n",
    "    \n",
    "    All sequences must have the same size. The array is filled with \n",
    "    END symbols as necessary.\n",
    "    \n",
    "    :param array_size: the array size expected by the encoder/decoder\n",
    "    :param sequence_size: the number of items (digits) actually\n",
    "        contained in the sequences. After that many entries, the array\n",
    "        is filled with the END symbol.\n",
    "    :param batch_size: number of sequences\n",
    "    \"\"\"\n",
    "    dims = (array_size, batch_size)\n",
    "    sequences = np.random.random_integers(0, 9, dims)\n",
    "    sequences[sequence_size:] = Symbol.END\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def run_network(input_sequence, sequence_size):\n",
    "    \"\"\"\n",
    "    Create a numpy array with the decoder outputs\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    batch_size = input_sequence.shape[1]\n",
    "    current_symbol = [Symbol.GO] * batch_size\n",
    "    \n",
    "    encoder_feeds = {first_term: input_sequence, \n",
    "                     first_term_size: sequence_size}\n",
    "    hidden_state = sess.run(state_1st_term, feed_dict=encoder_feeds)\n",
    "    \n",
    "    # this array control which sequences have already been finished by the\n",
    "    # decoder, i.e., for which ones it already produced the END symbol\n",
    "    sequences_done = np.zeros(batch_size, dtype=np.bool)\n",
    "    while True:\n",
    "        decoder_feeds = {decoder_step_state: hidden_state,\n",
    "                         digit_step: current_symbol}\n",
    "\n",
    "        fetches = sess.run([next_symbol, decoder_new_state], \n",
    "                           feed_dict=decoder_feeds)\n",
    "        current_symbol, hidden_state = fetches\n",
    "        \n",
    "        # use an \"additive\" or in order to avoid infinite loops\n",
    "        sequences_done |= (current_symbol == Symbol.END)\n",
    "                \n",
    "        if sequences_done.all():\n",
    "            break\n",
    "\n",
    "        answer.append(current_symbol)\n",
    "    \n",
    "    return np.vstack(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train and validation sets \n",
    "---\n",
    "\n",
    "We could just keep generating random data to train and evaluate the models, but using a predefined set allows us to compare different models over a common baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    \"\"\"\n",
    "    Return a copy of the array x without duplicate columns\n",
    "    \"\"\"\n",
    "    order = np.lexsort(x)\n",
    "    ordered = x[:, order]\n",
    "    diffs = np.diff(ordered, axis=1)\n",
    "    diff_sums = np.sum(np.abs(diffs), 0)\n",
    "    unique_indices = np.ones(x.shape[1], dtype='bool')\n",
    "    unique_indices[1:] = diff_sums != 0\n",
    "    unique_x = ordered[:, unique_indices]\n",
    "    \n",
    "    return unique_x\n",
    "    \n",
    "\n",
    "def shuffle_data_and_sizes(data, sizes):\n",
    "    \"\"\"\n",
    "    Convenient function for shuffling a dataset and its sizes with the same\n",
    "    RNG state.\n",
    "    \"\"\"\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(data.T)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(sizes)\n",
    "    \n",
    "    \n",
    "def generate_dataset(array_size, num_sequences, return_sizes=True):\n",
    "    \"\"\"\n",
    "    Generate one dataset as a 2-dim numpy array\n",
    "    \n",
    "    :param array_size: the array size expected by the network\n",
    "    :param num_sequences: the total number of sequences (columns) in the result\n",
    "    :param return_sizes: if True, returns a tuple with the dataset and\n",
    "        a 1-d array with the size of each sequence\n",
    "    \"\"\"\n",
    "    data = np.random.random_integers(0, 9, (array_size, num_sequences))\n",
    "    seq_sizes = np.empty(num_sequences, dtype=np.int)\n",
    "    \n",
    "    possible_sizes = np.arange(1, array_size + 1)\n",
    "    exps = np.exp(possible_sizes)\n",
    "    proportions = exps / np.sum(exps) * num_sequences\n",
    "    proportions = np.ceil(proportions).astype(np.int)\n",
    "    \n",
    "    last_idx = 0\n",
    "    for i, prop in enumerate(proportions, 1):\n",
    "        until_idx = last_idx + prop\n",
    "        \n",
    "        data[i:, last_idx:until_idx] = Symbol.END\n",
    "        seq_sizes[last_idx:until_idx] = i\n",
    "        \n",
    "        last_idx = until_idx\n",
    "    \n",
    "    if return_sizes:\n",
    "        return (data, seq_sizes)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(data, sizes, ignore_end=True):\n",
    "    \"\"\"\n",
    "    Get the prediciton accuracy on the supplied data.\n",
    "    \n",
    "    :param ignore_end: if True, ignore the END symbol\n",
    "    \"\"\"\n",
    "    answer = run_network(data, sizes)\n",
    "    \n",
    "    # if the answer is longer than it should, truncate it\n",
    "    if len(answer) > len(data):\n",
    "        answer = answer[:len(data)]\n",
    "    \n",
    "    hits = answer == data\n",
    "    total_items = answer.size\n",
    "        \n",
    "    if ignore_end:\n",
    "        non_end = data != Symbol.END\n",
    "        hits_non_end = hits[non_end]\n",
    "        total_items = np.sum(non_end)\n",
    "        \n",
    "    acc = np.sum(hits_non_end) / total_items\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80806278986799862"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(valid_set, valid_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 32000 sequences; 1000 for validation\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "train_size = 32000\n",
    "valid_size = 1000\n",
    "\n",
    "total_data = train_size + valid_size\n",
    "data, sizes = generate_dataset(input_sequence_size, total_data)\n",
    "shuffle_data_and_sizes(data, sizes)\n",
    "\n",
    "# removing duplicates must be change to account for the sizes.... at any rate, \n",
    "# we were getting 5 duplicates out of 32k. i don't think we really need it\n",
    "#data = remove_duplicates(data)\n",
    "train_set = data[:, :train_size]\n",
    "valid_set = data[:, train_size:]\n",
    "train_sizes = sizes[:train_size]\n",
    "valid_sizes = sizes[train_size:]\n",
    "\n",
    "n_train = train_set.shape[1]\n",
    "n_valid = valid_set.shape[1]\n",
    "print 'Training with %d sequences; %d for validation' % (n_train, n_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some important variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "accumulated_loss = 0\n",
    "report_interval = 100\n",
    "save_path = '../checkpoints/basic-memorizer.dat'\n",
    "\n",
    "num_batches = int(n_train / batch_size)\n",
    "\n",
    "saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 100\n",
      "Train loss: 2.00596\n",
      "Validation accuracy: 0.223626\n",
      "Epoch 1, batch 200\n",
      "Train loss: 1.09714\n",
      "Validation accuracy: 0.383359\n",
      "Epoch 1, batch 300\n",
      "Train loss: 0.69080\n",
      "Validation accuracy: 0.480153\n",
      "Epoch 1, batch 400\n",
      "Train loss: 0.55724\n",
      "Validation accuracy: 0.576350\n",
      "Epoch 1, batch 500\n",
      "Train loss: 0.47874\n",
      "Validation accuracy: 0.614734\n",
      "Epoch 1, batch 600\n",
      "Train loss: 0.41278\n",
      "Validation accuracy: 0.630111\n",
      "Epoch 1, batch 700\n",
      "Train loss: 0.36679\n",
      "Validation accuracy: 0.728931\n",
      "Epoch 1, batch 800\n",
      "Train loss: 0.30472\n",
      "Validation accuracy: 0.752414\n",
      "Epoch 1, batch 900\n",
      "Train loss: 0.27869\n",
      "Validation accuracy: 0.776851\n",
      "Epoch 1, batch 1000\n",
      "Train loss: 0.23319\n",
      "Validation accuracy: 0.807963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    # loop for epochs - each one goes through the whole dataset\n",
    "    shuffle_data_and_sizes(train_set, train_sizes)\n",
    "    last_batch_idx = 0\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        batch_idx = last_batch_idx + batch_size\n",
    "        batch = train_set[:, last_batch_idx:batch_idx]\n",
    "        sizes = train_sizes[last_batch_idx:batch_idx]\n",
    "        last_batch_idx = batch_idx\n",
    "    \n",
    "    #     # a single size for all items in the input\n",
    "    #     sequence_size = np.random.random_integers(1, input_sequence_size)\n",
    "    #     sequences = generate_sequences(input_sequence_size, sequence_size, batch_size)\n",
    "    #     sequence_size_array = np.array([sequence_size] * batch_size)\n",
    "    \n",
    "        feeds = {first_term: batch, \n",
    "                 first_term_size: sizes,\n",
    "                 l2_constant: 0.0001, \n",
    "                 learning_rate: 0.1}\n",
    "\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feeds)\n",
    "        accumulated_loss += loss_value\n",
    "    \n",
    "        if (batch_num + 1) % report_interval == 0:\n",
    "            print('Epoch %d, batch %d' % (epoch_num + 1, batch_num + 1))\n",
    "            avg_loss = accumulated_loss / report_interval\n",
    "            print('Train loss: %.5f' % avg_loss)\n",
    "            accumulated_loss = 0\n",
    "            \n",
    "            valid_acc = get_accuracy(valid_set, valid_sizes)\n",
    "            print('Validation accuracy: %f' % valid_acc)\n",
    "            \n",
    "        \n",
    "    print('')\n",
    "\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Saving\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../checkpoints/basic-memorizer.dat'"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=1)\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'embeddings:0',\n",
       " u'output_softmax/weights:0',\n",
       " u'output_softmax/bias:0',\n",
       " u'encoder/RNN/LSTMCell/W_0:0',\n",
       " u'encoder/RNN/LSTMCell/B:0',\n",
       " u'decoder/rnn_decoder/LSTMCell/W_0:0',\n",
       " u'decoder/rnn_decoder/LSTMCell/B:0',\n",
       " u'Variable:0']"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf.get_variable_scope().reuse_variables()\n",
    "x = tf.get_variable('encoder/RNN/LSTMCell/W_0')\n",
    "\n",
    "# with tf.variable_scope(encoder_scope, reuse=True) as scope:\n",
    "#     with tf.variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21284425451092118"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(valid_set, valid_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
